# Financial-Fraud-Detection-System
This module deploys a real-time Streamlit dashboard to visualize fraud metrics and transaction anomalies from the SQLite database . It utilizes pyngrok to tunnel the local server to a public URL, enabling remote monitoring of fraud distribution and high-risk alerts .


# ğŸ›¡ï¸ Financial Fraud Detection System

## ğŸ“Œ Project Overview

The **Financial Fraud Detection System** utilizes Machine Learning (ML) identify suspicious transactions and detect fraudulent activities in real-time. This system is designed for financial institutions to mitigate risks by processing transaction logs, identifying anomalies, and visualizing high-risk patterns via an interactive dashboard.

## ğŸš€ Key Features

* 
**Data Acquisition & ETL:** Extracts raw transaction data, transforms it, and loads it into a **SQLite** database for persistence.


* 
**Big Data Processing:** Uses **PySpark** for scalable data handling and feature engineering.


* 
**Fraud Detection Models:** Implements Supervised Learning (Logistic Regression, Random Forest) to classify transactions as valid or fraudulent.


* 
**Real-Time Monitoring:** Simulates streaming data ingestion (using Kafka/Spark Structured Streaming) to detect anomalies as they occur.


* 
**Interactive Dashboard:** A **Streamlit** web application accessible via **Ngrok** tunneling to visualize fraud statistics and alerts.


* 
**Alerting System:** Automated email alerts for high-risk transactions.



## ğŸ“‚ Project File Structure

```plaintext
â”œâ”€â”€ Financial_Fraud_Detection.ipynb   # Main Jupyter Notebook containing the full pipeline
â”œâ”€â”€ app.py                            # Streamlit dashboard application (generated by notebook)
â”œâ”€â”€ synthetic_fraud_dataset1.csv      # Raw input dataset (uploaded by user)
â”œâ”€â”€ fraud_detection.db                # Local SQLite database storing cleaned transactions
â”œâ”€â”€ fraud_model/                      # Directory containing the saved PySpark ML model
â””â”€â”€ README.md                         # Project documentation

```

## ğŸ› ï¸ Tech Stack

* **Core Engine:** Apache Spark (PySpark)
* **Language:** Python 3.x
* **Database:** SQLite (Local storage)
* **Libraries:** Pandas, NumPy, Matplotlib, Seaborn (Data Analysis & Visualization)
* **Dashboarding:** Streamlit, Pyngrok (UI & Tunneling)
* 
**Streaming (Simulated):** Apache Kafka (Optional/Conceptual) 



## âš™ï¸ Workflow Pipeline

The project follows a structured end-to-end pipeline:

1. **Environment Setup:** Installation of PySpark, Streamlit, and initialization of the Spark Session.
2. **ETL Process (Extract, Transform, Load):**
* Load raw `.csv` data using Pandas.
* Clean and normalize data.
* Store structured data into `fraud_detection.db`.




3. **Spark Integration:**
* Read data from SQLite/CSV into a Spark DataFrame.
* Define schemas (Transaction ID, Amount, Timestamp, etc.).




4. **Feature Engineering:**
* Vector Assembly of numerical features (Amount, Hour, Location).
* String Indexing for categorical variables.




5. **Model Training:**
* Train a **Logistic Regression** model using Spark MLlib.
* Evaluate using AUC and Confusion Matrix metrics.




6. **Dashboard Deployment:**
* Launch the `app.py` script using Streamlit.
* Expose the dashboard publicly using `ngrok` for remote monitoring.





## ğŸš¦ How to Run

1. **Open the Notebook:** Load `Financial_Fraud_Detection.ipynb` in Google Colab or Jupyter.
2. **Upload Data:** Ensure `synthetic_fraud_dataset1.csv` is in the file path.
3. **Execute Pipeline:** Run all cells sequentially to perform ETL and Model Training.
4. **Launch Dashboard:** Run the final section of the notebook to write `app.py` and start the Streamlit server via Ngrok.
5. **Access:** Click the `public_url` generated by Ngrok to view the real-time fraud monitor.

